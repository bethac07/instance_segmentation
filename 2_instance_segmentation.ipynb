{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xu8CArqlOlNi"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "import zarr\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "from imgaug.augmentables.heatmaps import HeatmapsOnImage\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.colormap import *\n",
    "from unet_fov import *\n",
    "from utils.mean_shift import MeanShift\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G0Yy0pbOlNm"
   },
   "source": [
    "\n",
    "Training an Instance Segmentation model\n",
    "==================================\n",
    "\n",
    "So far we were only interested in classes, what is background and foreground, \n",
    "where are cells or person vs car. But in many cases we not only want to know\n",
    "if a certain pixel belongs to a cell, but also to which cell.\n",
    "\n",
    "For isolated objects, this is trivial, all connected foreground pixels form\n",
    "one instance, yet often instances are very close together or even overlapping.\n",
    "Then we need to think a bit more how to formulate the loss for our network\n",
    "and how to extract the instances from the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-R1dHaMYOlNo"
   },
   "source": [
    "Data\n",
    "-------\n",
    "For this task we use a subset of the data used in the kaggle data science bowl 2018 challenge\n",
    "(https://www.kaggle.com/c/data-science-bowl-2018/)\n",
    "\n",
    "Example image:\n",
    "![image.png](utils/attachment/image.png)\n",
    "\n",
    "All images show nuclei recorded using different microscopes and lighting conditions.\n",
    "There are 30 images in the training set, 8 in the validation set and 16 in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Create the KaggleDSB_dataset\n",
    "-------\n",
    "We will create the KaggleDSB_dataset, a subclass which inherits from torch.utils.data.Dataset.\n",
    "\n",
    "When you just have limited number of data for training, data augmentation is essential to get good results.\n",
    "\n",
    "TODO: Implement the part of **define_augmentation** for training data during training on the fly.Think about what kind of augmentation to use (e.g. flips, rotation, elastic).Use the imgaug library (https://imgaug.readthedocs.io/en/latest/), it provides a very extensive list of available augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompress data\n",
    "from shutil import unpack_archive\n",
    "unpack_archive(os.path.join('datasets','data_kaggle.tar.gz'), './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vj8Atp9OlNo"
   },
   "outputs": [],
   "source": [
    "class KaggleDSB_dataset(Dataset):\n",
    "    \"\"\"(subset of the) kaggle data science bowl 2018 dataset.\n",
    "    The data is loaded from disk on the fly and in parallel using the torch dataset class.\n",
    "    This enables the use of datasets that would not fit into main memory and dynamic augmentation.\n",
    "    Args:\n",
    "        root_dir (string): Directory with all the images.\n",
    "        data_type (string): train/val/test, select subset of images\n",
    "        prediction_type (string): default to be \"metric_learning\" for this notebook\n",
    "        net_input_size (list): the input title size of you UNet\n",
    "        padding_size (int): the number of pixels to pad on each side of the image before augmentation and cropping\n",
    "        cache: if cache the data, default: False\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 data_type,\n",
    "                 prediction_type=\"two_class\",\n",
    "                 net_input_size=None,\n",
    "                 padding_size=None\n",
    "                ):\n",
    "        self.data_type = data_type\n",
    "        self.files = glob.glob(os.path.join(root_dir, data_type, \"*.zarr\"))\n",
    "        self.prediction_type = prediction_type\n",
    "        self.net_input_size = net_input_size\n",
    "        self.padding_size = padding_size\n",
    "        self.define_augmentation()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def define_augmentation(self):\n",
    "        \n",
    "        self.transform = iaa.Identity\n",
    "        self.crop = None\n",
    "        self.pad = None\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO (optional): Define your augmentation pipeline and uncomment the    #\n",
    "        # following code                                                          #\n",
    "        ###########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        # define self.transfrom by looking into the imgaug package reference\n",
    "        \n",
    "        # self.transform = iaa.Sequential([\n",
    "        #     ...,\n",
    "        #     ...,\n",
    "        #    ...\n",
    "        # ], random_order=True)\n",
    "        \n",
    "        \n",
    "        # if self.net_input_size is not None:\n",
    "        #     self.crop = ...\n",
    "\n",
    "        # if self.padding_size is not None:\n",
    "        #     self.pad =  ...\n",
    "            \n",
    "            \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    def get_filename(self, idx):\n",
    "        return self.files[idx]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.get_filename(idx)\n",
    "        raw, label = self.load_sample(fn)\n",
    "        raw = self.normalize(raw)\n",
    "        # augment for training\n",
    "        if self.padding_size is not None:\n",
    "            raw = self.pad(images = raw) # CHW -> CHW\n",
    "            label = self.pad(images = label) # CHW -> CHW\n",
    "        if self.data_type == \"train\":\n",
    "            raw = np.transpose(raw, [1,2,0]) # CHW -> HWC\n",
    "            label = np.transpose(label, [1,2,0]) # CHW -> HWC            \n",
    "            raw, label = self.augment_sample(raw, label) # HWC -> HWC\n",
    "            raw = np.transpose(raw, [2,0,1]) # HWC -> CHW\n",
    "            label = np.transpose(label, [2,0,1]) # HWC -> CHW\n",
    "        if self.net_input_size is not None:\n",
    "            tmp = np.concatenate([raw, label], axis = 0).copy() # C1+C2 HW\n",
    "            tmp = np.transpose(tmp, [1,2,0]) # CHW -> HWC \n",
    "            tmp = self.crop.augment_image(tmp) # HWC -> HWC\n",
    "            tmp = np.transpose(tmp, [2,0,1])\n",
    "            raw, label = np.expand_dims(tmp[0], axis=0), np.stack(tmp[1:],axis=0) # split\n",
    "        raw, label = torch.tensor(raw), torch.tensor(label)\n",
    "        return raw, label\n",
    "    \n",
    "    def augment_sample(self, raw, label):\n",
    "        # stores float label (sdt) differently than integer label (rest)\n",
    "        if self.prediction_type in [\"sdt\"]:\n",
    "            label = HeatmapsOnImage(label, shape=raw.shape, min_value=-1.0, max_value=1.0)\n",
    "            raw, label = self.transform(image=raw, heatmaps=label)\n",
    "        else:\n",
    "            label = label.astype(np.int32)\n",
    "            label = SegmentationMapsOnImage(label, shape=raw.shape)\n",
    "            raw, label = self.transform(image=raw, segmentation_maps=label)\n",
    "            \n",
    "        label = label.get_arr() \n",
    "        # some pytorch version have problems with negative indices introduced by e.g. flips\n",
    "        # just copying fixes this\n",
    "        label = label.copy()\n",
    "        raw = raw.copy()\n",
    "        return raw, label\n",
    "    \n",
    "    def normalize(self, raw):\n",
    "        # z-normalization\n",
    "        raw -= np.mean(raw)\n",
    "        raw /= np.std(raw)\n",
    "        return raw\n",
    "    \n",
    "    def load_sample(self, filename):\n",
    "        data = zarr.open(filename)\n",
    "        raw = np.array(data['volumes/raw'])\n",
    "        if self.prediction_type == \"two_class\":\n",
    "            label = np.array(data['volumes/gt_fgbg'])\n",
    "        elif self.prediction_type == \"affinities\":\n",
    "            label = np.array(data['volumes/gt_affs'])\n",
    "        elif self.prediction_type == \"sdt\":\n",
    "            label = np.array(data['volumes/gt_tanh'])\n",
    "        elif self.prediction_type == \"three_class\":\n",
    "            label = np.array(data['volumes/gt_threeclass'])\n",
    "        elif self.prediction_type == \"metric_learning\":\n",
    "            label = np.array(data['volumes/gt_labels'])\n",
    "        label = label.astype(np.float32)\n",
    "        return raw, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHbYAufiOlNs"
   },
   "source": [
    "Loss\n",
    "-------\n",
    "There are many different approaches to instance segmentation.\n",
    "We will introduce three basic methods:\n",
    "\n",
    "### Three-class model ###\n",
    "This is an extension of the basic foreground/background (or two-class) model.\n",
    "In addition a third class is introduced: the boundary.\n",
    "Even if two instances are touching, there is a boundary between them. This way they can be separated. \n",
    "Instead of a single output (where an output of zero is one class and of one is the other class), the network outputs three values, one per class. And the loss function changes from binary to (sparse) categorical cross entropy.\n",
    "\n",
    "![three_class.png](utils/attachment/three_class.png)\n",
    "\n",
    "### Distance Transform ###\n",
    "The label for each pixel is the distance to the closest boundary. \n",
    "The value within instances is negative and outside of instances is positive.\n",
    "As the output is not a probability but an (in principle) unbounded scalar, the mean squared error loss function is used.\n",
    "\n",
    "![sdt.png](utils/attachment/sdt.png)\n",
    "\n",
    "\n",
    "### Edge Affinities ###\n",
    "Here we consider not just the pixel but also its direct neighbors (in 2D the left neighbor and the upper neighbor are sufficient, right and down are redundant with the next pixel's left and upper neighbor).\n",
    "Imagine there is an edge between two pixels if they are in the same class and no edge if not. If we then take all pixels that are directly and indirectly connected by edges, we get an instance. The network predicts the probability that there is an edge, this is called affinity.\n",
    "As we are considering two neighbors per pixel, our network needs two outputs and as the output is a probability, we are using binary cross entropy\n",
    "\n",
    "![affinities.png](utils/attachment/affinities.png)\n",
    "\n",
    "### Metric Learning ###\n",
    "In metric learning your model learns to predict an embedding vector for each pixel. These embedding vectors are learned such that vectors from pixels belonging to the same instance are similar to each other and dissimilar to the embedding vectors of other instances and the background. It can also be thought of as learning a false coloring where each instance is colored with a unique but arbitrary color.  \n",
    "![metric_learning.png](utils/attachment/metric_learning.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Predefine some conditions for CNN training\n",
    "-------\n",
    "We have 5 types of labels, corresponding to 5 **prediction_types** below.\n",
    "\n",
    "For each case, we should define the corresponding output channel numbers, final activation layer, criterion(loss function) and dtye(the data type of the label).It would be clear to fill in these conditions after you look through the part of code about how we define training process.\n",
    "\n",
    "TODO: Please fill in the missing code and uncomment one of the **prediction_type** to start your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.disc_loss import DiscriminativeLoss\n",
    "\n",
    "###########################################################################\n",
    "# TODO: Uncomment the prediction_type (and corresponding conditions)      #\n",
    "#       you would like to use for this exercice                           #\n",
    "###########################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "# Uncomment to choose one\n",
    "\n",
    "prediction_type = \"two_class\" # same as fg/bg\n",
    "#prediction_type = \"affinities\"\n",
    "#prediction_type = \"sdt\"\n",
    "#prediction_type = \"three_class\"\n",
    "#prediction_type = \"metric_learning\"\n",
    "\n",
    "if prediction_type == \"two_class\":\n",
    "    out_channels = \n",
    "    activation =\n",
    "    loss_fn = \n",
    "    dtype = \n",
    "elif prediction_type == \"affinities\":\n",
    "    out_channels = \n",
    "    activation =\n",
    "    loss_fn = \n",
    "    dtype = \n",
    "elif prediction_type == \"sdt\":\n",
    "    out_channels =\n",
    "    activation = \n",
    "    loss_fn = \n",
    "    dtype = \n",
    "elif prediction_type == \"three_class\":\n",
    "    out_channels = \n",
    "    activation = \n",
    "    loss_fn =\n",
    "    dtype = \n",
    "elif prediction_type == \"metric_learning\":\n",
    "    out_channels = \n",
    "    activation = \n",
    "    loss_fn = \n",
    "    dtype = \n",
    "else:\n",
    "    raise RuntimeError(\"invalid prediction type\")\n",
    "    \n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPSW17KmOlNu"
   },
   "source": [
    "Create our input datasets, ground truth labels are chosen depending on the type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VuVxLBhOlNv"
   },
   "outputs": [],
   "source": [
    "# make datasets\n",
    "root = 'data_kaggle_test'\n",
    "padding_size = 12\n",
    "batch_size = 4\n",
    "\n",
    "data_train = KaggleDSB_dataset(root, \"train\", prediction_type=prediction_type, padding_size=padding_size)\n",
    "data_val = KaggleDSB_dataset(root, \"val\", prediction_type=prediction_type, padding_size=padding_size)\n",
    "data_test = KaggleDSB_dataset(root, \"test\", prediction_type=prediction_type, padding_size=padding_size)\n",
    "# make dataloaders\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(data_val, batch_size=1, pin_memory=True)\n",
    "test_loader = DataLoader(data_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faMWkovLOlNw"
   },
   "source": [
    "Let's have a look at some of the raw data and labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWP4xCcmOlNx",
    "outputId": "54372c21-625e-49be-ce03-ba133a2c7642"
   },
   "outputs": [],
   "source": [
    "# repeatedly execute this cell to get different images\n",
    "for image, label in data_train:\n",
    "    break\n",
    "\n",
    "label = np.squeeze(label, 0)\n",
    "if prediction_type == \"affinities\":\n",
    "    label = label[0] + label[1]\n",
    "\n",
    "fig=plt.figure(figsize=(12, 8))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(np.squeeze(image), cmap='gray')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(np.squeeze(label), cmap='gist_earth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBbq0Ek6OlNy"
   },
   "source": [
    "### Receptive Field of View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmlqVRH_OlNy",
    "outputId": "31f3a721-116e-4906-9d15-e9c0da1a381e"
   },
   "outputs": [],
   "source": [
    "images, labels = data_train[random.randrange(len(data_train))]\n",
    "rnd = random.randrange(len(images))\n",
    "image = images[rnd]\n",
    "label = labels[rnd]\n",
    "\n",
    "\n",
    "net_t = image\n",
    "fovs = []\n",
    "d_factors = [[2,2],[2,2],[2,2]]\n",
    "\n",
    "net = UNet(in_channels=1,\n",
    "           num_fmaps=6,\n",
    "           fmap_inc_factors=2,\n",
    "           downsample_factors=d_factors,\n",
    "           padding='same',\n",
    "           num_fmaps_out=out_channels\n",
    "          )\n",
    "\n",
    "for level in range(len(d_factors)+1):\n",
    "    fov_tmp, _ = net.rec_fov(level , (1, 1), 1)\n",
    "    fovs.append(fov_tmp[0])\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "colors = [\"yellow\", \"red\", \"green\", \"blue\", \"magenta\"]\n",
    "plt.imshow(image, cmap='gray')\n",
    "for idx, fov_t in enumerate(fovs):\n",
    "    print(\"Field of view at depth {}: {:3d} (color: {})\".format(idx+1, fov_t, colors[idx]))\n",
    "    xmin = image.shape[1]/2 - fov_t/2\n",
    "    xmax = image.shape[1]/2 + fov_t/2\n",
    "    ymin = image.shape[1]/2 - fov_t/2\n",
    "    ymax = image.shape[1]/2 + fov_t/2\n",
    "    plt.hlines(ymin, xmin, xmax, color=colors[idx], lw=3)\n",
    "    plt.hlines(ymax, xmin, xmax, color=colors[idx], lw=3)\n",
    "    plt.vlines(xmin, ymin, ymax, color=colors[idx], lw=3)\n",
    "    plt.vlines(xmax, ymin, ymax, color=colors[idx], lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Define our U-Net\n",
    "==============\n",
    "As before, we define our neural network architecture and can choose the depth and number of feature maps at the first convolution.\n",
    "\n",
    "This neural network is composed by stacking one UNet instance and one convolution layer. The UNet Class is defined in the **unet_fov.py** file and we use the default setting that the number of feature maps of the UNet instance output will be eqaul to the number of feature maps at the first convolution. Then we use one more convolution layer with kernel_size=1 to generate the final output with number of feature maps equal to what we want.\n",
    "\n",
    "For the meaning of parameters of the UNet class, please refer to the **unet_fov.py** file.\n",
    "\n",
    "#TODO: Please fill in the missing part of code about how to define the **net**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "###########################################################################\n",
    "# TODO: Define the net and uncomment the following code                   #\n",
    "# Please define a UNet which use same padding                             #\n",
    "###########################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "'''\n",
    "The explanation of the arguments for the UNet instances are shown below. \n",
    "For more detailed instruction, please look at unet_fov.py.\n",
    "Args:\n",
    "\n",
    "    in_channels:\n",
    "\n",
    "        The number of input channels.\n",
    "\n",
    "    num_fmaps:\n",
    "\n",
    "        The number of feature maps in the first layer. This is also the\n",
    "        number of output feature maps. Stored in the ``channels``\n",
    "        dimension.\n",
    "\n",
    "    fmap_inc_factors:\n",
    "\n",
    "        By how much to multiply the number of feature maps between\n",
    "        layers. If layer 0 has ``k`` feature maps, layer ``l`` will\n",
    "        have ``k*fmap_inc_factor**l``.\n",
    "\n",
    "    downsample_factors:\n",
    "\n",
    "        List of tuples ``(y, x)`` to use to down- and up-sample the\n",
    "        feature maps between layers.\n",
    "\n",
    "\n",
    "    activation:\n",
    "\n",
    "        Which activation to use after a convolution. Accepts the name\n",
    "        of any tensorflow activation function (e.g., ``ReLU`` for\n",
    "        ``torch.nn.ReLU``).\n",
    "\n",
    "    constant_upsample (optional):\n",
    "\n",
    "        If set to true, perform a constant upsampling instead of a\n",
    "        transposed convolution in the upsampling layers.\n",
    "'''\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#d_factors = \n",
    "\n",
    "#net = torch.nn.Sequential(\n",
    "#     UNet(in_channels=,\n",
    "#     num_fmaps=,\n",
    "#     fmap_inc_factors=d_factors,\n",
    "#     downsample_factors=,\n",
    "#     activation='ReLU',\n",
    "#     padding=,\n",
    "#     num_fmaps_out=,\n",
    "#     constant_upsample=\n",
    "#     ),\n",
    "#     torch.nn.Conv2d(in_channels= , out_channels=out_channels, kernel_size=1, padding=0, bias=True))\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "summary(net, (1, 384, 384))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk6vI6hAOlN0"
   },
   "source": [
    "#TODO: Training\n",
    "=======\n",
    "\n",
    "Before we start training, we have to compile the network and set the optimizer (try playing with the learning rate, a higher learning rate can lead to faster training, but also to divergence or lower performance).\n",
    "\n",
    "To visualize our results we now use Tensorboard. This is a very useful extension for your browser that let's you look into networks computational graph and the weights and metrics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npfIvMJPOlN0"
   },
   "outputs": [],
   "source": [
    "def training_step(model, loss_fn, optimizer, feature, label, prediction_type):\n",
    "    # speedup version of setting gradients to zero\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    # forward\n",
    "    logits = model(feature) # B x C x H x W\n",
    "    shape_dif = np.array(label.shape[-2:]) - np.array(logits.shape[-2:])\n",
    "    if np.sum(shape_dif)>0:\n",
    "        label = label[:,:,shape_dif[0]//2:-shape_dif[0]//2,shape_dif[0]//2:-shape_dif[0]//2]        \n",
    "    if prediction_type == \"three_class\":\n",
    "        label=torch.squeeze(label,1) #label.shape=[N,H,W]\n",
    "    loss_value = loss_fn(input=logits, target=label)  #logits.shape=[N,C,H,W] label.shape=[N,H,W]\n",
    "    # backward if training mode\n",
    "    if net.training:\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    if activation is not None:\n",
    "        output = activation(logits)\n",
    "    else:\n",
    "        output = logits\n",
    "    outputs = {\n",
    "        'pred': output,\n",
    "        'logits': logits,\n",
    "    }\n",
    "    return loss_value, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "33b3c69443ba4e909a23cbdf6f196601"
     ]
    },
    "id": "m_PNIU-jOlN0",
    "outputId": "7f3e2d66-4a29-4dbf-c171-b222d05449fa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_steps = 2000\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "writer = SummaryWriter(logdir)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "###########################################################################\n",
    "# TODO: put the model and the loss function to device and define the      #\n",
    "# optimizer with its learning rate                                        #\n",
    "###########################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set flags\n",
    "net.train() \n",
    "loss_fn.train()\n",
    "step = 0\n",
    "\n",
    "# this might take ~10 ish minutes\n",
    "with tqdm(total=training_steps) as pbar:\n",
    "    while step < training_steps:\n",
    "        # reset data loader to get random augmentations\n",
    "        np.random.seed()\n",
    "        tmp_loader = iter(train_loader)\n",
    "        for feature, label in tmp_loader:\n",
    "            label = label.type(dtype)\n",
    "            label = label.to(device)\n",
    "            feature = feature.to(device)\n",
    "            loss_value, pred = training_step(net, loss_fn, optimizer, feature, label, prediction_type)\n",
    "            writer.add_scalar('loss',loss_value.cpu().detach().numpy(),step)\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "            if step % 100 == 0:\n",
    "                net.eval()\n",
    "                tmp_val_loader = iter(val_loader)\n",
    "                acc_loss = []\n",
    "                for feature, label in tmp_val_loader:                    \n",
    "                    label = label.type(dtype)\n",
    "                    label = label.to(device)\n",
    "                    feature = feature.to(device)\n",
    "                    loss_value, _ = training_step(net, loss_fn, optimizer, feature, label, prediction_type)\n",
    "                    acc_loss.append(loss_value.cpu().detach().numpy())\n",
    "                writer.add_scalar('val_loss',np.mean(acc_loss),step)\n",
    "                net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78tq1yX2OlN1",
    "outputId": "3ed28cf9-baed-4062-920e-463811789a8f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "\n",
    "#or run:\n",
    "#!tensorboard --logdir=runs \n",
    "#to view in separate window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrsSO59gOlN1"
   },
   "source": [
    "Postprocessing\n",
    "=============\n",
    "\n",
    "In contrast to the semantic segmentation we postprocessing to extract the final segmentation is a bit more involved and consists of x steps for the two class, three class, sdt and affinity models:\n",
    "- based on the prediction we define a surface\n",
    "- we extract the maxima from this surface\n",
    "- we use the maxima as seeds in an off-the-shelf watershed algorithm\n",
    "- and mask the result with the foreground\n",
    "The foreground areas covered by the watershed from each seed point correspond to the instances.\n",
    "The resulting instances are then matched to the ground truth instances (at least 50% overlap) to get our final score (averaged over all instances and all test images)\n",
    "\n",
    "For the metric learning model, the post-processing is a bit different. The embeddings are clustered with the mean shift algorithm and the clusters are numbered. You can think of this as clustering pixels by their color, such that the pixels that belong to one uniquely colored instance end up in one cluster and get the same number assigned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyU505GiOlN2",
    "outputId": "f3617f71-4467-40ec-d267-98d21a5989e1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.label import *\n",
    "from utils.evaluate import *\n",
    "\n",
    "# set flag\n",
    "net.eval()\n",
    "# set hyperparameters\n",
    "# thresholds have to be tuned after training on the validation set\n",
    "if prediction_type == \"two_class\":\n",
    "    fg_thresh = 0.7\n",
    "    seed_thresh = 0.8\n",
    "elif prediction_type == \"three_class\":\n",
    "    #pred = np.transpose(pred, [2, 0, 1])\n",
    "    fg_thresh = 0.5\n",
    "    seed_thresh = 0.6\n",
    "elif prediction_type == \"sdt\":\n",
    "    fg_thresh = 0.0\n",
    "    seed_thresh = -0.12\n",
    "elif prediction_type == \"affinities\":\n",
    "    fg_thresh = 0.9\n",
    "    seed_thresh = 0.99\n",
    "elif prediction_type == \"metric_learning\":\n",
    "    fg_thresh = 0.5\n",
    "    seed_thresh = None\n",
    "    \n",
    "def unpad(pred, padding_size):\n",
    "    return pred[padding_size:-padding_size,padding_size:-padding_size]\n",
    "\n",
    "avg = 0.0\n",
    "for idx, (image, gt_labels) in enumerate(test_loader):\n",
    "    image = image.to(device)\n",
    "    pred = net(image)\n",
    "    image = np.squeeze(image.cpu())\n",
    "    gt_labels = np.squeeze(gt_labels)\n",
    "    pred = np.squeeze(pred.cpu().detach().numpy(),0)\n",
    "    #if prediction_type in [\"three_class\", \"affinities\",\"two_class\",\"sdt\"]:\n",
    "    pred = unpad(np.transpose(pred,(1,2,0)), padding_size)\n",
    "    pred = np.transpose(pred,(2,0,1))\n",
    "    if prediction_type == \"affinities\":\n",
    "        gt_labels = gt_labels[0] + gt_labels[1]\n",
    "    labelling, surface = label(pred, prediction_type, fg_thresh=fg_thresh, seed_thresh=seed_thresh)\n",
    "    ap, precision, recall, tp, fp, fn = evaluate(labelling, data_test.get_filename(idx))\n",
    "    avg += ap\n",
    "    print(np.min(surface), np.max(surface))\n",
    "    labelling = labelling.astype(np.uint8)\n",
    "    print(\"average precision: {}, precision: {}, recall: {}\".format(ap, precision, recall))\n",
    "    print(\"true positives: {}, false positives: {}, false negatives: {}\".format(tp, fp, fn))\n",
    "    if prediction_type == \"metric_learning\":\n",
    "        surface = surface+np.abs(np.min(surface, axis=(1,2)))[:,np.newaxis,np.newaxis]\n",
    "        surface /= np.max(surface, axis=(1,2))[:,np.newaxis,np.newaxis]\n",
    "        surface = np.transpose(surface, (1,2,0))\n",
    "\n",
    "    fig=plt.figure(figsize=(16, 8))\n",
    "    ax = fig.add_subplot(1, 4, 1)\n",
    "    ax.set_title(\"raw\")\n",
    "   \n",
    "    image=unpad(image,(image.shape[-1]-labelling.shape[-1])//2)\n",
    "    plt.imshow(np.squeeze(image))\n",
    "    ax = fig.add_subplot(1, 4, 2)\n",
    "    ax.set_title(\"gt labels\")\n",
    "    gt_labels=unpad(gt_labels,(gt_labels.shape[-1]-labelling.shape[-1])//2)\n",
    "    plt.imshow(np.squeeze(1.0-gt_labels))\n",
    "    \n",
    "    ax = fig.add_subplot(1, 4, 3)\n",
    "    ax.set_title(\"prediction\")\n",
    "    plt.imshow(np.squeeze(1.0-surface))\n",
    "    ax = fig.add_subplot(1, 4, 4)\n",
    "    ax.set_title(\"pred segmentation\")\n",
    "    plt.imshow(np.squeeze(labelling), cmap=rand_cmap, interpolation=\"none\")\n",
    "\n",
    "    plt.show()\n",
    "avg /= (idx+1)\n",
    "print(\"average precision on test set: {}\".format(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different methods, hyperparameters, and see how you can improve the results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "instance_segmentation.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "cff0e7dafc57c8272ee1343dbc442356272010b158bded75ebcd02aba72f7245"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
