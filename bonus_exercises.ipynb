{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411bab38",
   "metadata": {},
   "source": [
    "# Bonus exercises and further learning\n",
    "\n",
    "- These exercises do not have todos, feel free to run them to get a sense of a few more tricks you can use for instance segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34575b-5688-48ed-93ff-f50f55e13aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "from glob import glob\n",
    "from skimage import color\n",
    "from skimage.io import imread\n",
    "from natsort import natsorted\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "from unet import *\n",
    "from utils import *\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820db827-198b-465b-941c-22d3452f3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start by loading our images into lists so we can visualize and get oriented with our data\n",
    "# natsorted is a package that takes away some of the annoyances of the regular sorting function\n",
    "# glob is a package that allows us to load files from directories. Feel free to inspect these lists.\n",
    "\n",
    "train_cyto = natsorted(glob('woodshole/train/*cyto*'))\n",
    "train_nuclei = natsorted(glob('woodshole/train/*nuclei*'))\n",
    "train_raw = [i for i in natsorted(glob('woodshole/train/*.tif')) if 's.tif' not in i]\n",
    "\n",
    "test_cyto = natsorted(glob('woodshole/test/*cyto*'))\n",
    "test_nuclei = natsorted(glob('woodshole/test/*nuclei*'))\n",
    "test_raw = [i for i in natsorted(glob('woodshole/test/*.tif')) if 's.tif' not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223540b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the decive we'll be using throughout the notebook\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7fbe07-8be2-426f-b256-cc8a20e48b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random cytoplasm mask file\n",
    "cyto_file = random.choice(train_cyto)\n",
    "\n",
    "# use skimage.io.imread to read our data into numpy arrays\n",
    "cyto = imread(cyto_file)\n",
    "nuclei = imread(cyto_file.replace('cyto', 'nuclei'))\n",
    "raw = imread(cyto_file.replace('_cyto_masks', ''))\n",
    "\n",
    "#our raw data shape is (c, h, w) and there are only two channels.\n",
    "#to visualize as an rgb image we need to add another dummy dimension and then transpose so that it is (h,w,3)\n",
    "raw = np.vstack((raw, np.zeros_like(raw)[:1]))\n",
    "raw = raw.transpose(1,2,0)\n",
    "\n",
    "# visualize the data - execute this cell a few times to see different examples.\n",
    "# you can also change train_cyto to test_cyto above to see some test data. it is pretty similar \n",
    "fig, axes = plt.subplots(1,5,figsize=(20, 10),sharex=True,sharey=True,squeeze=False)\n",
    "\n",
    "axes[0][0].imshow(raw[:,:,0], cmap='gray')\n",
    "axes[0][0].title.set_text('Raw nuclei channel')\n",
    "\n",
    "axes[0][1].imshow(raw[:,:,1], cmap='gray')\n",
    "axes[0][1].title.set_text('Raw cyto channel')\n",
    "\n",
    "axes[0][2].imshow(raw)\n",
    "axes[0][2].title.set_text('Raw overlay')\n",
    "\n",
    "axes[0][3].imshow(raw[:,:,0], cmap='gray')\n",
    "axes[0][3].imshow(create_lut(nuclei), alpha=0.5)\n",
    "axes[0][3].title.set_text('nuclei mask')\n",
    "\n",
    "axes[0][4].imshow(raw[:,:,1], cmap='gray')\n",
    "axes[0][4].imshow(create_lut(cyto), alpha=0.5)\n",
    "axes[0][4].title.set_text('cyto mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6b809-d894-4b0a-a383-ac95ff1edd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TissueNetDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 split='train',\n",
    "                 mask='nuclei',\n",
    "                 crop_size=None,\n",
    "                 val_split=False,\n",
    "                 prediction_type='two_class',\n",
    "                 padding_size=8\n",
    "                ):\n",
    "        \n",
    "        self.split = split\n",
    "        self.mask_files = natsorted(glob(os.path.join(root_dir, split, f'*{mask}*')))\n",
    "        self.raw_files = [i for i in natsorted(glob(os.path.join(root_dir, split, '*.tif'))) if 's.tif' not in i]\n",
    "        self.crop_size = crop_size\n",
    "        self.prediction_type = prediction_type\n",
    "        self.padding_size = padding_size\n",
    "        \n",
    "        if split == 'test':\n",
    "            if val_split:\n",
    "                self.mask_files = self.mask_files[:10]\n",
    "                self.raw_files = self.raw_files[:10]\n",
    "            else:\n",
    "                self.mask_files = self.mask_files[10:]\n",
    "                self.raw_files = self.raw_files[10:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_files)\n",
    "    \n",
    "    def get_padding(self, crop_size, padding_size):\n",
    "    \n",
    "        # quotient\n",
    "        q = int(crop_size / padding_size)\n",
    "    \n",
    "        if crop_size % padding_size != 0:\n",
    "            padding = (padding_size * (q + 1))\n",
    "        else:\n",
    "            padding = crop_size\n",
    "    \n",
    "        return padding\n",
    "    \n",
    "    def create_target(self, mask, prediction_type):\n",
    "        \n",
    "        mask, border = erode_border(\n",
    "                    mask,\n",
    "                    iterations=1,\n",
    "                    border_value=1)\n",
    "        \n",
    "        if self.prediction_type == 'two_class':\n",
    "            mask = (mask != 0)\n",
    "\n",
    "        elif self.prediction_type == 'three_class':\n",
    "            labels_two_class = (mask != 0)\n",
    "            border[border!=0] = 2\n",
    "            \n",
    "            mask = labels_two_class + border\n",
    "\n",
    "        elif self.prediction_type == 'sdt':\n",
    "            mask = compute_sdt(mask)\n",
    "\n",
    "        elif self.prediction_type == 'affs':\n",
    "            mask = compute_affinities(mask, nhood=[[0,1],[1,0]])\n",
    "\n",
    "        else:\n",
    "            raise Exception('Choose from one of the following prediction types: two_class, three_class, sdt, affs')\n",
    "        \n",
    "        return mask.astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_file = self.raw_files[idx]\n",
    "        mask_file = self.mask_files[idx]\n",
    "        \n",
    "        raw = imread(raw_file)\n",
    "        mask = imread(mask_file)\n",
    "        \n",
    "        raw = raw.transpose([1,2,0]).astype(np.float32)\n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            padding = self.get_padding(self.crop_size, self.padding_size)\n",
    "            raw, mask = augment_data(raw, mask, padding, self.crop_size)\n",
    "            \n",
    "        raw = raw.transpose([2,0,1])\n",
    "        mask = mask.transpose([2,0,1])\n",
    "        \n",
    "        mask = self.create_target(mask[0], self.prediction_type)\n",
    "        \n",
    "        if self.prediction_type != 'affs':\n",
    "            mask = np.expand_dims(mask, axis=0)\n",
    "                                        \n",
    "        return raw, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ace09-2b95-4ad8-bde0-483486c16818",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Auxiliary learning\n",
    "\n",
    "- Auxiliary learning is a powerful technique that can help to improve the results of our main objective by providing a helper task. Up until now, we have only shown our model representations of the data that are boundary specific. But the data is a lot richer than that - these objects have distinct shapes that could be leveraged in order to better learn the boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b593025-1e9a-4cf1-96ef-c177a7c450a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"height:2px;\"><div class=\"alert alert-block alert-info\"><h3>Cellpose</h3>\n",
    "    \n",
    "- In [**Cellpose**](https://cellpose.readthedocs.io/en/latest/), cells are turned into flow representations. We create these flow representations by simulating diffusion from the center of the cell to get the spatial gradients for each pixel that point towards the center of the cell. During test time, we use the flows as a dynamical system and all pixels that converge to the same point are defined as the pixels in a given cell. The flows shown below are represented by an HSV colormap used in the optic flow literature.\n",
    "    \n",
    "    \n",
    "- We also predict the foregroud / background -- the two classes you predicted in exercise 1. In Cellpose we call this the cell probability. We threshold this to decide which pixels are in cells -- we only use these pixels to run the dynamical system.\n",
    "    \n",
    "    \n",
    "- The flow representation allows the learning of non-convex shapes, because pixels can flow around corners. It also prevents merging, as flows for two cells that are touching are opposite.\n",
    "</div>\n",
    "\n",
    "![cellpose_flows](static/cellpose_flows.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e880baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "\n",
    "# create a cellpose model on the gpu\n",
    "# use a built-in model trained on tissuenet\n",
    "# (the first time you run this cell the model will download)\n",
    "model = models.CellposeModel(gpu=device, model_type='tissuenet')\n",
    "\n",
    "test_dataset = TissueNetDataset(root_dir='woodshole', split='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "### IMPORTANT: these are the channels used for the segmentation\n",
    "# the first one is the channel to segment, and the second one is the optional nuclear channel\n",
    "# red = 1\n",
    "# green = 2\n",
    "# blue = 3\n",
    "\n",
    "channels = [2, 1]\n",
    "\n",
    "masks_cp = []\n",
    "for idx, (image, mask) in enumerate(test_loader):\n",
    "    image = image.cpu().detach().numpy()\n",
    "    mask_cp, flows, styles = model.eval(image, diameter=25, channels=channels)\n",
    "    masks_cp.append(mask_cp)\n",
    "    \n",
    "    fig, axes = plt.subplots(1,4,figsize=(20, 20),sharex=True,sharey=True,squeeze=False)\n",
    "    \n",
    "    image = np.squeeze(image)\n",
    "    image = np.vstack((image, np.zeros_like(image)[:1]))\n",
    "    image = image.transpose(1,2,0)\n",
    "    \n",
    "    axes[0][0].imshow(image)\n",
    "    axes[0][0].title.set_text('Raw')\n",
    "    \n",
    "    axes[0][1].imshow(create_lut(mask_cp))\n",
    "    axes[0][1].title.set_text('Predicted Labels')\n",
    "\n",
    "    axes[0][2].imshow(flows[0])\n",
    "    axes[0][2].title.set_text('Predicted cellpose')\n",
    "    \n",
    "    axes[0][3].imshow(flows[2])\n",
    "    axes[0][3].title.set_text('Predicted cell probability')\n",
    "    \n",
    "    \n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also use the nuclear channel ONLY and run a nuclear model in cellpose\n",
    "# we set the second channel = 0 because we do not have an additional channel now\n",
    "channels = [1, 0]\n",
    "\n",
    "# initialize nuclei model (can also try \"cyto\" model if this doesn't work)\n",
    "# the \"nuclei\" model in cellpose has been trained on lots of nuclear data (but not the tissuenet dataset)\n",
    "# the \"cyto\" model in cellpose has been trained on many cellular images (but not the tissuenet dataset)\n",
    "model = models.CellposeModel(gpu=device, model_type='nuclei')\n",
    "\n",
    "masks_cp = []\n",
    "for idx, (image, mask) in enumerate(test_loader):\n",
    "    image = image.cpu().detach().numpy()\n",
    "    mask_cp, flows, styles = model.eval(image, diameter=20, channels=channels)\n",
    "    masks_cp.append(mask_cp)\n",
    "    \n",
    "    fig, axes = plt.subplots(1,4,figsize=(20, 20),sharex=True,sharey=True,squeeze=False)\n",
    "    \n",
    "    image = np.squeeze(image)\n",
    "    image = np.vstack((image, np.zeros_like(image)[:1]))\n",
    "    image = image.transpose(1,2,0)\n",
    "    \n",
    "    axes[0][0].imshow(image)\n",
    "    axes[0][0].title.set_text('Raw')\n",
    "    \n",
    "    axes[0][1].imshow(create_lut(mask_cp))\n",
    "    axes[0][1].title.set_text('Predicted Labels')\n",
    "\n",
    "    axes[0][2].imshow(flows[0])\n",
    "    axes[0][2].title.set_text('Predicted cellpose')\n",
    "    \n",
    "    axes[0][3].imshow(flows[2])\n",
    "    axes[0][3].title.set_text('Predicted cell probability')\n",
    "    \n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593600a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"height:2px;\"><div class=\"alert alert-block alert-info\"><h3>Local Shape Descriptors</h3>\n",
    "\n",
    "- Another example of auxiliary learning is [**LSDs**](https://localshapedescriptors.github.io/). This embedding encodes object shape similarly but is computed in a defined gaussian constrained to each label. This allows for consistent gradients regardless of object shapes which makes it a good candidate for segmentation of complex objects such as neurons in large electron microscopy datasets. \n",
    "\n",
    "\n",
    "- The LSDs are combined with nearest neighbor affinities to improve the boundary representations. The improved affinities then produce nice segmentations when using a hierarchical agglomeration approach and can be easily parallelized to allow for scaling to massive volumes. \n",
    "\n",
    "![example_image](static/lsd_schematic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf570532-444e-411b-b452-7576f873d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lsds, calculate on a small patch and visualize the descriptor components\n",
    "\n",
    "from lsd.train import local_shape_descriptor\n",
    "\n",
    "file = random.choice(train_nuclei)\n",
    "\n",
    "nuclei = imread(file)[0:64, 0:64]\n",
    "raw = imread(file.replace('_nuclei_masks', ''))[:, 0:64, 0:64]\n",
    "\n",
    "#just to visualize\n",
    "raw = np.vstack((raw, np.zeros_like(raw)[:1]))\n",
    "raw = raw.transpose(1,2,0)\n",
    "\n",
    "lsds = local_shape_descriptor.get_local_shape_descriptors(\n",
    "              segmentation=nuclei,\n",
    "              sigma=(5,)*2,\n",
    "              voxel_size=(1,)*2)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "            1,\n",
    "            6,\n",
    "            figsize=(20, 20),\n",
    "            sharex=False,\n",
    "            sharey=True,\n",
    "            squeeze=False)\n",
    "  \n",
    "axes[0][0].imshow(np.squeeze(lsds[0]), cmap='jet')\n",
    "axes[0][0].title.set_text('Mean offset Y')\n",
    "\n",
    "axes[0][1].imshow(np.squeeze(lsds[1]), cmap='jet')\n",
    "axes[0][1].title.set_text('Mean offset X')\n",
    "\n",
    "axes[0][2].imshow(np.squeeze(lsds[2]), cmap='jet')\n",
    "axes[0][2].title.set_text('Covariance Y-Y')\n",
    "\n",
    "axes[0][3].imshow(np.squeeze(lsds[3]), cmap='jet')\n",
    "axes[0][3].title.set_text('Covariance X-X')\n",
    "\n",
    "axes[0][4].imshow(np.squeeze(lsds[4]), cmap='jet')\n",
    "axes[0][4].title.set_text('Covariance Y-X')\n",
    "\n",
    "axes[0][5].imshow(np.squeeze(lsds[5]), cmap='jet')\n",
    "axes[0][5].title.set_text('Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f27af-1f0d-4d1e-a519-1ea4666793e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly modify our dataset just for simplicity\n",
    "\n",
    "class TissueNetDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 split='train',\n",
    "                 mask='nuclei',\n",
    "                 crop_size=None,\n",
    "                 val_split=False,\n",
    "                 padding_size=8\n",
    "                ):\n",
    "        \n",
    "        self.split = split\n",
    "        self.mask_files = natsorted(glob(os.path.join(root_dir, split, f'*{mask}*')))\n",
    "        self.raw_files = [i for i in natsorted(glob(os.path.join(root_dir, split, '*.tif'))) if 's.tif' not in i]\n",
    "        self.crop_size = crop_size\n",
    "        self.padding_size = padding_size\n",
    "        \n",
    "        if split == 'test':\n",
    "            if val_split:\n",
    "                self.mask_files = self.mask_files[:10]\n",
    "                self.raw_files = self.raw_files[:10]\n",
    "            else:\n",
    "                self.mask_files = self.mask_files[10:]\n",
    "                self.raw_files = self.raw_files[10:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_files)\n",
    "    \n",
    "    def get_padding(self, crop_size, padding_size):\n",
    "    \n",
    "        # quotient\n",
    "        q = int(crop_size / padding_size)\n",
    "    \n",
    "        if crop_size % padding_size != 0:\n",
    "            padding = (padding_size * (q + 1))\n",
    "        else:\n",
    "            padding = crop_size\n",
    "    \n",
    "        return padding\n",
    "    \n",
    "    def augment_data(self, raw, mask, padding):\n",
    "        \n",
    "        transform = A.Compose([\n",
    "              A.RandomCrop(\n",
    "                  width=self.crop_size,\n",
    "                  height=self.crop_size),\n",
    "              A.PadIfNeeded(\n",
    "                  min_height=padding,\n",
    "                  min_width=padding,\n",
    "                  p=1,\n",
    "                  border_mode=0),\n",
    "              A.HorizontalFlip(p=0.3),\n",
    "              A.VerticalFlip(p=0.3),\n",
    "              A.RandomRotate90(p=0.3),\n",
    "              A.Transpose(p=0.3),\n",
    "              A.RandomBrightnessContrast(p=0.3)\n",
    "            ])\n",
    "\n",
    "        transformed = transform(image=raw, mask=mask)\n",
    "\n",
    "        raw, mask = transformed['image'], transformed['mask']\n",
    "        \n",
    "        return raw, mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_file = self.raw_files[idx]\n",
    "        mask_file = self.mask_files[idx]\n",
    "        \n",
    "        raw = imread(raw_file)\n",
    "        mask = imread(mask_file)\n",
    "\n",
    "        raw = raw.transpose([1,2,0])\n",
    "        \n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        mask = mask.transpose([1,2,0])\n",
    "                \n",
    "        # just do this regardless of split to make val/test faster for demo purposes\n",
    "        padding = self.get_padding(self.crop_size, self.padding_size)\n",
    "        raw, mask = self.augment_data(raw, mask, padding)\n",
    "            \n",
    "        raw = raw.transpose([2,0,1])\n",
    "        mask = mask.transpose([2,0,1])\n",
    "        \n",
    "        mask, border = erode_border(\n",
    "                    mask[0],\n",
    "                    iterations=1,\n",
    "                    border_value=1)\n",
    "\n",
    "        affs = compute_affinities(mask, nhood=[[0,1],[1,0]])\n",
    "                        \n",
    "        lsds = local_shape_descriptor.get_local_shape_descriptors(\n",
    "              segmentation=mask,\n",
    "              sigma=(5,)*2,\n",
    "              voxel_size=(1,)*2)\n",
    "\n",
    "        lsds = lsds.astype(np.float32)\n",
    "        affs = affs.astype(np.float32)\n",
    "                                        \n",
    "        return raw, lsds, affs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d178b0-b3a6-438c-ac67-5ba357a45a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize batch\n",
    "\n",
    "train_dataset = TissueNetDataset(root_dir='woodshole', split='train', crop_size=64)\n",
    "\n",
    "raw, lsds, affs = train_dataset[random.randrange(len(train_dataset))]\n",
    "\n",
    "raw = np.vstack((raw, np.zeros_like(raw)[:1]))\n",
    "raw = raw.transpose(1,2,0)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "            1,\n",
    "            7,\n",
    "            figsize=(20, 20),\n",
    "            sharex=False,\n",
    "            sharey=True,\n",
    "            squeeze=False)\n",
    "  \n",
    "axes[0][0].imshow(np.squeeze(lsds[0]), cmap='jet')\n",
    "axes[0][0].title.set_text('Mean offset Y')\n",
    "\n",
    "axes[0][1].imshow(np.squeeze(lsds[1]), cmap='jet')\n",
    "axes[0][1].title.set_text('Mean offset X')\n",
    "\n",
    "axes[0][2].imshow(np.squeeze(lsds[2]), cmap='jet')\n",
    "axes[0][2].title.set_text('Covariance Y-Y')\n",
    "\n",
    "axes[0][3].imshow(np.squeeze(lsds[3]), cmap='jet')\n",
    "axes[0][3].title.set_text('Covariance X-X')\n",
    "\n",
    "axes[0][4].imshow(np.squeeze(lsds[4]), cmap='jet')\n",
    "axes[0][4].title.set_text('Covariance Y-X')\n",
    "\n",
    "axes[0][5].imshow(np.squeeze(lsds[5]), cmap='jet')\n",
    "axes[0][5].title.set_text('Size')\n",
    "\n",
    "axes[0][6].imshow(np.squeeze(affs[0]+affs[1]), cmap='jet')\n",
    "axes[0][6].title.set_text('Affs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fe179-8ed2-45d3-b0b6-d03f761bffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need two output heads for our network, one for lsds and one for affinities\n",
    "# to do this we will subclass torch.nn.Module and create our UNet inside\n",
    "# before we had a single final convolution. Now we have one for each head.\n",
    "# then in the forward pass we pass our image through our unet and then the output through each head\n",
    "\n",
    "class MtlsdModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        num_fmaps,\n",
    "        fmap_inc_factors,\n",
    "        downsample_factors,\n",
    "        padding='same'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.unet = UNet(\n",
    "            in_channels=in_channels,\n",
    "            num_fmaps=num_fmaps,\n",
    "            fmap_inc_factors=fmap_inc_factors,\n",
    "            downsample_factors=downsample_factors,\n",
    "            padding=padding)\n",
    "\n",
    "        self.lsd_head = torch.nn.Conv2d(in_channels=num_fmaps,out_channels=6, kernel_size=1)\n",
    "        self.aff_head = torch.nn.Conv2d(in_channels=num_fmaps,out_channels=2, kernel_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        z = self.unet(input)\n",
    "        lsds = self.lsd_head(z)\n",
    "        affs = self.aff_head(z)\n",
    "\n",
    "        return lsds, affs\n",
    "\n",
    "# We want to combine the lsds and affs losses and minimize the sum\n",
    "# we can do this by subclassing our loss function (torch.nn.MSELoss) and overriding the forward method\n",
    "\n",
    "class CombinedLoss(torch.nn.MSELoss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "\n",
    "    def forward(self, lsds_prediction, lsds_target, affs_prediction, affs_target):\n",
    "\n",
    "        loss1 = super(CombinedLoss, self).forward(lsds_prediction,lsds_target)\n",
    "        loss2 = super(CombinedLoss, self).forward(affs_prediction, affs_target)\n",
    "        \n",
    "        return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eeeabc-09f4-4570-90fb-c4784d9c31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "d_factors = [[2,2],[2,2],[2,2]]\n",
    "\n",
    "in_channels=2\n",
    "num_fmaps=32\n",
    "fmap_inc_factors=4\n",
    "\n",
    "net = MtlsdModel(in_channels,num_fmaps,fmap_inc_factors,d_factors)\n",
    "\n",
    "loss_fn = CombinedLoss().to(device)\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a39b6-5c07-42bb-acde-10fcdea0a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 3000\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "net = net.to(device)\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "# set optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# set activation\n",
    "activation = torch.nn.Sigmoid()\n",
    "\n",
    "### create datasets\n",
    "\n",
    "train_dataset = TissueNetDataset(root_dir='woodshole', split='train', crop_size=64)\n",
    "test_dataset = TissueNetDataset(root_dir='woodshole', split='test', crop_size=128)\n",
    "val_dataset = TissueNetDataset(root_dir='woodshole', split='test', val_split=True, crop_size=64)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# make dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191c382-6301-46ae-84ec-cdaa54d11c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update our training step to have two logits and two predictions\n",
    "\n",
    "def model_step(model, loss_fn, optimizer, feature, gt_lsds, gt_affs, activation, train_step=True):\n",
    "    \n",
    "    # zero gradients if training\n",
    "    if train_step:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # forward\n",
    "    lsd_logits, affs_logits = model(feature)\n",
    "\n",
    "    loss_value = loss_fn(lsd_logits, gt_lsds, affs_logits, gt_affs)\n",
    "    \n",
    "    # backward if training mode\n",
    "    if train_step:\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    lsd_output = activation(lsd_logits)\n",
    "    affs_output = activation(affs_logits)\n",
    "   \n",
    "    outputs = {\n",
    "        'pred_lsds': lsd_output,\n",
    "        'pred_affs': affs_output,\n",
    "        'lsds_logits': lsd_logits,\n",
    "        'affs_logits': affs_logits,\n",
    "    }\n",
    "    \n",
    "    return loss_value, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0fb35-41e6-4ee9-a550-45ba27fb143e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update our training loop to do both lsds and affs\n",
    "\n",
    "# set flags\n",
    "net.train() \n",
    "loss_fn.train()\n",
    "step = 0\n",
    "\n",
    "with tqdm(total=training_steps) as pbar:\n",
    "    while step < training_steps:\n",
    "        # reset data loader to get random augmentations\n",
    "        np.random.seed()\n",
    "        tmp_loader = iter(train_loader)\n",
    "        for feature, gt_lsds, gt_affs in tmp_loader:\n",
    "            gt_lsds = gt_lsds.to(device)\n",
    "            gt_affs = gt_affs.to(device)\n",
    "            feature = feature.to(device)\n",
    "                        \n",
    "            #print(label.shape, feature.shape)\n",
    "                    \n",
    "            loss_value, pred = model_step(net, loss_fn, optimizer, feature, gt_lsds, gt_affs, activation)\n",
    "            writer.add_scalar('loss',loss_value.cpu().detach().numpy(),step)\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                net.eval()\n",
    "                tmp_val_loader = iter(test_loader)\n",
    "                acc_loss = []\n",
    "                for feature, gt_lsds, gt_affs in tmp_val_loader:                    \n",
    "                    gt_lsds = gt_lsds.to(device)\n",
    "                    gt_affs = gt_affs.to(device)\n",
    "                    feature = feature.to(device)\n",
    "                    loss_value, _ = model_step(net, loss_fn, optimizer, feature, gt_lsds, gt_affs, activation, train_step=False)\n",
    "                    acc_loss.append(loss_value.cpu().detach().numpy())\n",
    "                writer.add_scalar('val_loss',np.mean(acc_loss),step) \n",
    "                net.train()\n",
    "                print(np.mean(acc_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636afc0-7bed-4808-9c66-5dc52ba95796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize a few predictions - have the lsds helped to improve the affinities?\n",
    "# For a future challenge you could try using a weighted combined loss and watershed + agglomeration to get strong segmentations\n",
    "\n",
    "net.eval()\n",
    "\n",
    "activation = torch.nn.Sigmoid()\n",
    "\n",
    "for idx, (image, gt_lsds, gt_affs) in enumerate(test_loader):\n",
    "    image = image.to(device)\n",
    "    lsds_logits, affs_logits = net(image)\n",
    "    pred_lsds = activation(lsds_logits)\n",
    "    pred_affs = activation(affs_logits)\n",
    "        \n",
    "    image = np.squeeze(image.cpu())\n",
    "    gt_lsds = np.squeeze(gt_lsds.cpu().numpy())\n",
    "    gt_affs = np.squeeze(gt_affs.cpu().numpy())\n",
    "    \n",
    "    pred_lsds = np.squeeze(pred_lsds.cpu().detach().numpy())\n",
    "    pred_affs = np.squeeze(pred_affs.cpu().detach().numpy())\n",
    "    \n",
    "    fig, axes = plt.subplots(1,3,figsize=(20, 20),sharex=True,sharey=True,squeeze=False)\n",
    "    \n",
    "    image = np.vstack((image, np.zeros_like(image)[:1]))\n",
    "    image = image.transpose(1,2,0)\n",
    "    \n",
    "    axes[0][0].imshow(image)\n",
    "    axes[0][0].title.set_text('Raw')\n",
    "  \n",
    "    axes[0][1].imshow(np.squeeze(pred_lsds[0]), cmap='jet')\n",
    "    axes[0][1].imshow(np.squeeze(pred_lsds[1]), cmap='jet', alpha=0.5)\n",
    "    axes[0][1].title.set_text('Mean offsets')\n",
    "\n",
    "    axes[0][2].imshow(np.squeeze(pred_affs[0]+pred_affs[1]), cmap='jet')\n",
    "    axes[0][2].title.set_text('Affs')\n",
    "    \n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63a89b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Further learning\n",
    "\n",
    "* Instance segmentation can be challenging and this exercise just scratches the surface of what is possible.\n",
    "\n",
    "\n",
    "* This notebook assumes images that fit into memory but often times this is not the case (especially in biology). \n",
    "    1. To see an example for predicting over an image in chunks and stitching the results together, see this [notebook](https://github.com/dlmbl/instance_segmentation/blob/2021-solutions/3_tile_and_stitch.ipynb)\n",
    "    2. For a more advanced library that makes it easier to do machine learning on massive datasets, see gunpowder (navigate to the tutorials, or browse the API): https://funkelab.github.io/gunpowder\n",
    "    \n",
    "    \n",
    "* We did not cover more complex loss functions. Here are some nice explanations / implementations of other loss functions that are useful for instance segmentation: https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook\n",
    "\n",
    "\n",
    "* A more complex (but powerful) approach is called metric learning. This can be seen in last years [exercise](https://github.com/dlmbl/instance_segmentation/blob/2021-solutions/2_instance_segmentation.ipynb)\n",
    "\n",
    "\n",
    "* We did not cover stardist in this tutorial, and barely scratched the surface on cellpose and lsds. For more tutorials on:\n",
    "    1. Stardist: https://github.com/maweigert/tutorials/tree/main/stardist\n",
    "    2. CellPose: https://github.com/MouseLand/cellpose#run-cellpose-10-without-local-python-installation\n",
    "    3. LSDs: https://github.com/funkelab/lsd#notebooks\n",
    "    \n",
    "### Good luck on your instance segmentation endeavors!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
